{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzc4OrQXWI77"
      },
      "outputs": [],
      "source": [
        "#1. Install & Import Dependencies\n",
        "# Colab cell 1: Install necessary packages\n",
        "!pip install yt-dlp\n",
        "!pip install scenedetect\n",
        "!pip install openai                 # needed by OpenRouter if using OpenAI client\n",
        "!pip install aiofiles fastapi uvicorn python-multipart  # for optional local hosting\n",
        "!pip install huggingface_hub        # HuggingFace Inference client\n",
        "!pip install requests               # simpler HTTP calls\n",
        "!pip install ffmpeg-python          # wrapper around ffmpeg\n",
        "!pip install faster-whisper         # CPU Whisper alternative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WPygR4IcTuR"
      },
      "source": [
        "Why these?\n",
        "\n",
        "yt-dlp for downloading YouTube shorts.\n",
        "\n",
        "ffmpeg-python (and native ffmpeg) to extract frames/audio.\n",
        "\n",
        "scenedetect for optional scene detection.\n",
        "\n",
        "huggingface_hub to call BLIP-2 & Whisper Inference endpoints.\n",
        "\n",
        "faster_whisper if you want purely local CPU transcription.\n",
        "\n",
        "requests to call OpenRouter LLMs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBnhpQq9WchF"
      },
      "outputs": [],
      "source": [
        "#1. Install & Import Dependencies\n",
        "# Colab cell 2: Import libraries\n",
        "import os, shutil, json, base64, uuid\n",
        "import yt_dlp\n",
        "import ffmpeg                       # ffmpeg-python\n",
        "import requests\n",
        "import subprocess\n",
        "from scenedetect import VideoManager, SceneManager\n",
        "from scenedetect.detectors import ContentDetector\n",
        "from huggingface_hub import InferenceApi\n",
        "from faster_whisper import WhisperModel\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yt-dlp"
      ],
      "metadata": {
        "id": "8GRG1lZDPGmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yt_dlp\n",
        "import os, shutil, json, base64, uuid"
      ],
      "metadata": {
        "id": "H8Ky4PqqPH73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XP9jPtZHW8US"
      },
      "outputs": [],
      "source": [
        "#2. Set Up API Keys & Folders\n",
        "# Colab cell 3: Environment variables (replace with your own)\n",
        "HF_API_KEY = \"\" #add your api\n",
        "OPENROUTER_API_KEY = \"\" #add your api\n",
        "\n",
        "# For Colab, you can securely store keys via:\n",
        "#   from google.colab import auth\n",
        "#   auth.authenticate_user()\n",
        "#   os.environ[\"HF_API_KEY\"] = HF_API_KEY\n",
        "#   os.environ[\"OPENROUTER_API_KEY\"] = OPENROUTER_API_KEY\n",
        "\n",
        "# Create working directories (clear if they exist)\n",
        "ROOT = \"/content/video_pipeline\"\n",
        "VIDEOS_DIR = f\"{ROOT}/videos\"\n",
        "FRAMES_DIR = f\"{ROOT}/frames\"\n",
        "OUTPUT_DIR = f\"{ROOT}/output\"\n",
        "\n",
        "for d in [VIDEOS_DIR, FRAMES_DIR, OUTPUT_DIR]:\n",
        "    if os.path.exists(d):\n",
        "        shutil.rmtree(d)\n",
        "    os.makedirs(d, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbzGyeZ5cV5I"
      },
      "source": [
        "Purpose:\n",
        "\n",
        "VIDEOS_DIR holds downloaded or uploaded videos.\n",
        "\n",
        "FRAMES_DIR holds extracted frames or scene snapshots.\n",
        "\n",
        "OUTPUT_DIR holds transcripts, captions, analysis JSONs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuoVL4ZMYC-x"
      },
      "outputs": [],
      "source": [
        "# 3. Ingest Video (YouTube URL or Local File)\n",
        "# Colab cell 4: Function to download video via yt-dlp\n",
        "def download_video(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Download a YouTube URL (short or regular) into VIDEOS_DIR.\n",
        "    Returns local path to the downloaded video.\n",
        "    \"\"\"\n",
        "    video_id = str(uuid.uuid4())[:8]\n",
        "    out_path = f\"{VIDEOS_DIR}/{video_id}.mp4\"\n",
        "    # yt-dlp prefers no \"-f best\" to auto‐select best combined format:\n",
        "    ytdl_opts = {\n",
        "        \"format\": \"bestvideo[ext=mp4]+bestaudio[ext=m4a]/mp4\",\n",
        "        \"outtmpl\": out_path,\n",
        "        \"quiet\": True\n",
        "    }\n",
        "    with yt_dlp.YoutubeDL(ytdl_opts) as ydl:\n",
        "        ydl.download([url])\n",
        "    return out_path\n",
        "\n",
        "# Example usage:\n",
        "video_path = download_video(\"https://youtu.be/F_Wp-rYKix4?si=er3dNva6sHESDhxi\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F53IAasYY_Ja"
      },
      "outputs": [],
      "source": [
        "# Colab cell 5: Alternatively, if you have a local file:\n",
        "def copy_local_video(local_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Copy an existing local video file into VIDEOS_DIR and return new path.\n",
        "    \"\"\"\n",
        "    base = os.path.basename(local_path)\n",
        "    dest = f\"{VIDEOS_DIR}/{str(uuid.uuid4())}_{base}\"\n",
        "    shutil.copy(local_path, dest)\n",
        "    return dest\n",
        "\n",
        "# Example usage:\n",
        "# video_path = copy_local_video(\"/content/1.mp4\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty0SEy5Mcfpm"
      },
      "source": [
        "Note: If yt-dlp fails (some Shorts restrictions), let the user supply a local file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNcxOsOdZfk4"
      },
      "outputs": [],
      "source": [
        "# 4. Scene/Frame Extraction\n",
        "# Option A: Fixed‐Interval Frames\n",
        "# Colab cell 6A: Extract fixed-interval frames (e.g. every N seconds)\n",
        "def extract_frames_interval(video_path: str, interval: float = 2.0) -> list[str]:\n",
        "    \"\"\"\n",
        "    Extracts frames every 'interval' seconds from video_path into FRAMES_DIR.\n",
        "    Returns list of frame file paths.\n",
        "    \"\"\"\n",
        "    # Create a unique subfolder for this video’s frames\n",
        "    vid_id = os.path.splitext(os.path.basename(video_path))[0]\n",
        "    folder = f\"{FRAMES_DIR}/{vid_id}\"\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "    # Use ffmpeg to extract one frame every 'interval' seconds:\n",
        "    (\n",
        "        ffmpeg\n",
        "        .input(video_path)\n",
        "        .filter('fps', fps=f'1/{interval}')\n",
        "        .output(f\"{folder}/frame_%04d.jpg\", start_number=1)\n",
        "        .overwrite_output()\n",
        "        .run(quiet=True)\n",
        "    )\n",
        "    # List sorted frame paths:\n",
        "    frames = sorted([f\"{folder}/{f}\" for f in os.listdir(folder) if f.endswith(\".jpg\")])\n",
        "    return frames\n",
        "\n",
        "# Example:\n",
        "# frames = extract_frames_interval(video_path, interval=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTRzJkuZaVwv"
      },
      "outputs": [],
      "source": [
        "# # Option B: Scene‐Change Detection (Optional)\n",
        "# # Colab cell 6B: Extract keyframes by scene detection\n",
        "# from scenedetect import VideoManager, SceneManager\n",
        "# from scenedetect.detectors import ContentDetector\n",
        "\n",
        "# def extract_frames_scenes(video_path: str) -> list[str]:\n",
        "#     \"\"\"\n",
        "#     Uses PySceneDetect to find scene changes and save the first frame of each scene.\n",
        "#     Returns list of saved frame file paths.\n",
        "#     \"\"\"\n",
        "#     vid_manager = VideoManager([video_path])\n",
        "#     scene_manager = SceneManager()\n",
        "#     scene_manager.add_detector(ContentDetector(threshold=30.0))  # adjust threshold\n",
        "\n",
        "#     vid_manager.start()\n",
        "#     scene_manager.detect_scenes(frame_source=vid_manager)\n",
        "#     scene_list = scene_manager.get_scene_list()\n",
        "#     vid_manager.release()\n",
        "\n",
        "#     frame_paths = []\n",
        "#     vid_filename = os.path.splitext(os.path.basename(video_path))[0]\n",
        "#     out_folder = f\"{FRAMES_DIR}/{vid_filename}\"\n",
        "#     os.makedirs(out_folder, exist_ok=True)\n",
        "\n",
        "#     for idx, (start, end) in enumerate(scene_list):\n",
        "#         cap = ffmpeg.input(video_path)\n",
        "#         time = start.get_seconds()\n",
        "#         # Seek to start and extract one frame\n",
        "#         out_file = f\"{out_folder}/scene_{idx+1:03d}.jpg\"\n",
        "#         (\n",
        "#             ffmpeg\n",
        "#             .input(video_path, ss=time)\n",
        "#             .filter('scale', -1, 360)  # resize to height 360 px if you like\n",
        "#             .output(out_file, vframes=1)\n",
        "#             .overwrite_output()\n",
        "#             .run(quiet=True)\n",
        "#         )\n",
        "#         frame_paths.append(out_file)\n",
        "#     return frame_paths\n",
        "\n",
        "# # Example:\n",
        "# # frames = extract_frames_scenes(video_path)\n",
        "\n",
        "# Option B: Scene‐Change Detection (Updated)\n",
        "# Colab cell 6B: Extract keyframes by scene detection (Modern PySceneDetect API)\n",
        "\n",
        "from scenedetect import open_video, SceneManager\n",
        "from scenedetect.detectors import ContentDetector\n",
        "import ffmpeg\n",
        "import os\n",
        "\n",
        "# Replace this with your actual frames directory constant\n",
        "FRAMES_DIR = f\"{ROOT}/frames\"\n",
        "\n",
        "def extract_frames_scenes(video_path: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Uses PySceneDetect to find scene changes and save the first frame of each scene.\n",
        "    Returns list of saved frame file paths.\n",
        "    \"\"\"\n",
        "    # Open the video using the modern PySceneDetect interface\n",
        "    video = open_video(video_path)\n",
        "    scene_manager = SceneManager()\n",
        "    scene_manager.add_detector(ContentDetector(threshold=30.0))  # Adjust threshold as needed\n",
        "\n",
        "    # Run scene detection\n",
        "    scene_manager.detect_scenes(video)\n",
        "    scene_list = scene_manager.get_scene_list()\n",
        "\n",
        "    # Close the video\n",
        "    video.close()\n",
        "\n",
        "    # Prepare output directory\n",
        "    vid_filename = os.path.splitext(os.path.basename(video_path))[0]\n",
        "    out_folder = os.path.join(FRAMES_DIR, vid_filename)\n",
        "    os.makedirs(out_folder, exist_ok=True)\n",
        "\n",
        "    # Extract one frame at the start of each detected scene\n",
        "    frame_paths = []\n",
        "    timestamps = []\n",
        "    for idx, (start, end) in enumerate(scene_list):\n",
        "        time = start.get_seconds()\n",
        "        out_file = f\"{out_folder}/scene_{idx+1:03d}.jpg\"\n",
        "        (\n",
        "            ffmpeg\n",
        "            .input(video_path, ss=time)\n",
        "            .filter('scale', -1, 360)  # Resize height to 360px while keeping aspect ratio\n",
        "            .output(out_file, vframes=1)\n",
        "            .overwrite_output()\n",
        "            .run(quiet=True)\n",
        "        )\n",
        "        frame_paths.append(out_file)\n",
        "        timestamps.append(time)\n",
        "\n",
        "    return frame_paths, timestamps\n",
        "\n",
        "# # Example usage:\n",
        "# # frames = extract_frames_scenes(video_path)\n",
        "\n",
        "# Option B: Scene‐Change Detection (Fixed)\n",
        "# from scenedetect import open_video, SceneManager\n",
        "# from scenedetect.detectors import ContentDetector\n",
        "# import ffmpeg\n",
        "# import os\n",
        "\n",
        "# FRAMES_DIR = \"frames\"  # Update if needed\n",
        "\n",
        "# def extract_frames_scenes(video_path: str) -> list[str]:\n",
        "#     \"\"\"\n",
        "#     Uses PySceneDetect to find scene changes and save the first frame of each scene.\n",
        "#     Returns list of saved frame file paths.\n",
        "#     \"\"\"\n",
        "#     # Open video\n",
        "#     video = open_video(video_path)\n",
        "#     scene_manager = SceneManager()\n",
        "#     scene_manager.add_detector(ContentDetector(threshold=30.0))  # Adjust as needed\n",
        "\n",
        "#     # Run scene detection\n",
        "#     scene_manager.detect_scenes(video)\n",
        "#     scene_list = scene_manager.get_scene_list()\n",
        "\n",
        "#     # video.close()  # ❌ Not needed (and not supported in newer PySceneDetect)\n",
        "\n",
        "#     # Prepare output directory\n",
        "#     vid_filename = os.path.splitext(os.path.basename(video_path))[0]\n",
        "#     out_folder = os.path.join(FRAMES_DIR, vid_filename)\n",
        "#     os.makedirs(out_folder, exist_ok=True)\n",
        "\n",
        "#     # Extract one frame at start of each scene\n",
        "#     frame_paths = []\n",
        "#     for idx, (start, end) in enumerate(scene_list):\n",
        "#         time = start.get_seconds()\n",
        "#         out_file = f\"{out_folder}/scene_{idx+1:03d}.jpg\"\n",
        "#         (\n",
        "#             ffmpeg\n",
        "#             .input(video_path, ss=time)\n",
        "#             .filter('scale', -1, 360)\n",
        "#             .output(out_file, vframes=1)\n",
        "#             .overwrite_output()\n",
        "#             .run(quiet=True)\n",
        "#         )\n",
        "#         frame_paths.append(out_file)\n",
        "\n",
        "#     return frame_paths\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8_AtsSLclnH"
      },
      "source": [
        "Which to choose?\n",
        "\n",
        "If you want uniform spacing: use 6A.\n",
        "\n",
        "If you want semantic breaks: use 6B."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XF04Wc8nbTEn"
      },
      "outputs": [],
      "source": [
        "# 5. (Optional) Extract & Transcribe Audio\n",
        "# A. Extract Audio with ffmpeg\n",
        "# Colab cell 7A: Extract audio track as WAV (16 kHz mono)\n",
        "def extract_audio(video_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts audio track (mono, 16 kHz) to WAV file and returns path.\n",
        "    \"\"\"\n",
        "    vid_id = os.path.splitext(os.path.basename(video_path))[0]\n",
        "    audio_path = f\"{VIDEOS_DIR}/{vid_id}.wav\"\n",
        "    (\n",
        "        ffmpeg\n",
        "        .input(video_path)\n",
        "        .output(audio_path, acodec='pcm_s16le', ac=1, ar='16000')\n",
        "        .overwrite_output()\n",
        "        .run(quiet=True)\n",
        "    )\n",
        "    return audio_path\n",
        "\n",
        "# Example:\n",
        "# audio_path = extract_audio(video_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWkW_ovtbh5G"
      },
      "outputs": [],
      "source": [
        "# B. Transcribe with faster-whisper (Local, CPU)\n",
        "# Colab cell 7B: Transcribe with faster-whisper\n",
        "def transcribe_audio_local(audio_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Uses faster-whisper to transcribe audio locally (CPU).\n",
        "    Returns the full transcript as text.\n",
        "    \"\"\"\n",
        "    model = WhisperModel(\"small\", device=\"cuda\")  # 'small' is fast; can choose 'tiny' for speed\n",
        "    segments, info = model.transcribe(audio_path, beam_size=5)\n",
        "    transcript = \" \".join([seg.text for seg in segments])\n",
        "    return transcript\n",
        "\n",
        "# Example:\n",
        "# transcript = transcribe_audio_local(audio_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl_GrUxHcs1v"
      },
      "source": [
        "Note: On Colab, you could use device=\"cuda\", but on your HP 15s, use cpu.\n",
        "\n",
        "Which to pick?\n",
        "\n",
        "Local Whisper is CPU‐heavy but free.\n",
        "\n",
        "HF Inference is faster for short audio (free tier/N credits/day)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rl5pzjeZcBaw"
      },
      "outputs": [],
      "source": [
        "# C. Transcribe with HuggingFace Inference (API)\n",
        "# Colab cell 7C: Transcribe using HuggingFace Whisper via API\n",
        "def transcribe_audio_hf(audio_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Sends raw WAV audio to HuggingFace Whisper-large API (free tier).\n",
        "    Returns transcript text.\n",
        "    \"\"\"\n",
        "    hf = InferenceApi(repo_id=\"openai/whisper-large\", token=HF_API_KEY)\n",
        "    # Read the WAV file as bytes\n",
        "    with open(audio_path, \"rb\") as f:\n",
        "        data = f.read()\n",
        "    result = hf(inputs=data)\n",
        "    # API response: { \"text\": \"...\" }\n",
        "    return result.get(\"text\", \"\")\n",
        "\n",
        "# Example:\n",
        "# transcript = transcribe_audio_hf(audio_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L42bP3r6cxlO"
      },
      "outputs": [],
      "source": [
        "# # 6. Caption Frames (BLIP-2 / Image2Prompt)\n",
        "# # A. BLIP-2 via HuggingFace Inference\n",
        "# # Colab cell 8A: Caption single image using BLIP-2 HF API\n",
        "# def caption_image_blip(frame_path: str) -> str:\n",
        "#     \"\"\"\n",
        "#     Sends a frame JPEG to HuggingFace BLIP-2 for captioning.\n",
        "#     Returns the generated caption text.\n",
        "#     \"\"\"\n",
        "#     hf = InferenceApi(repo_id=\"Salesforce/blip2-opt-2.7b\", token=HF_API_KEY)\n",
        "#     with open(frame_path, \"rb\") as f:\n",
        "#         img_bytes = f.read()\n",
        "#     response = hf(inputs=img_bytes)\n",
        "#     # response example: [\"a cat sitting on a sofa...\"]\n",
        "#     return response[0] if isinstance(response, list) else str(response)\n",
        "\n",
        "# # Batch caption all frames:\n",
        "# def batch_caption(frames: list[str]) -> dict[int,str]:\n",
        "#     captions = {}\n",
        "#     for idx, fp in enumerate(frames):\n",
        "#         cap = caption_image_blip(fp)\n",
        "#         captions[idx] = cap\n",
        "#     return captions\n",
        "\n",
        "# # Example:\n",
        "# # captions = batch_caption(frames)\n",
        "# 🔍 6. Caption Frames (BLIP-2 / Image2Prompt)\n",
        "# A. BLIP-2 via HuggingFace Inference (Fixed with direct request)\n",
        "\n",
        "# import requests\n",
        "# import json\n",
        "\n",
        "# def caption_image_blip(frame_path: str, hf_token: str) -> str:\n",
        "#     \"\"\"\n",
        "#     Sends a frame JPEG to HuggingFace BLIP-2 for captioning via direct requests.\n",
        "#     Returns the generated caption text.\n",
        "#     Requires HF_API_KEY to be passed.\n",
        "#     \"\"\"\n",
        "#     api_url = \"https://api-inference.huggingface.co/models/Salesforce/blip2-opt-2.7b\"\n",
        "#     headers = {\n",
        "#         \"Authorization\": f\"Bearer {hf_token}\"\n",
        "#     }\n",
        "\n",
        "#     with open(frame_path, \"rb\") as f:\n",
        "#         files = {\n",
        "#             \"file\": (frame_path, f, \"image/jpeg\")\n",
        "#         }\n",
        "#         response = requests.post(api_url, headers=headers, files=files)\n",
        "\n",
        "#     if response.status_code != 200:\n",
        "#         print(f\"HF API error {response.status_code}: {response.text}\")\n",
        "#         try:\n",
        "#             error_data = response.json()\n",
        "#             raise Exception(f\"HF API error {response.status_code}: {error_data.get('error', response.text)}\")\n",
        "#         except json.JSONDecodeError:\n",
        "#             raise Exception(f\"HF API error {response.status_code}: Could not decode response as JSON. Response text: {response.text}\")\n",
        "\n",
        "#     result = response.json()\n",
        "#     if isinstance(result, list) and result:\n",
        "#         return result[0]\n",
        "#     elif isinstance(result, dict) and \"error\" in result:\n",
        "#         raise Exception(f\"HF API returned error: {result['error']}\")\n",
        "#     else:\n",
        "#         return str(result)\n",
        "\n",
        "def caption_image_blip(frame_path: str, hf_token: str) -> str:\n",
        "    \"\"\"\n",
        "    Uses Hugging Face Inference API to caption an image via BLIP-2.\n",
        "    \"\"\"\n",
        "    url = \"https://api-inference.huggingface.co/models/Salesforce/blip2-opt-2.7b\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {hf_token}\"\n",
        "    }\n",
        "\n",
        "    with open(frame_path, \"rb\") as f:\n",
        "        image_bytes = f.read()\n",
        "\n",
        "    response = requests.post(url, headers=headers, data=image_bytes)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"[HF API Error] {frame_path} -> {response.status_code}: {response.text}\")\n",
        "        return \"Caption generation failed\"\n",
        "\n",
        "    try:\n",
        "        result = response.json()\n",
        "        if isinstance(result, dict) and \"generated_text\" in result:\n",
        "            return result[\"generated_text\"]\n",
        "        elif isinstance(result, str):\n",
        "            return result\n",
        "        else:\n",
        "            return json.dumps(result)\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"[JSON Error] Could not decode response: {response.text}\")\n",
        "        return \"Caption generation failed\"\n",
        "\n",
        "\n",
        "# 🚀 Batch caption all frames\n",
        "# def batch_caption(frames: list[str], hf_token: str) -> dict[int, str]:\n",
        "#     \"\"\"\n",
        "#     Batch captions frames using the updated caption_image_blip function.\n",
        "#     \"\"\"\n",
        "#     captions = {}\n",
        "#     for idx, fp in enumerate(frames):\n",
        "#         try:\n",
        "#             cap = caption_image_blip(fp, hf_token)\n",
        "#             captions[idx] = cap\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error captioning frame {idx}: {e}\")\n",
        "#             captions[idx] = \"Captioning failed\"\n",
        "#     return captions\n",
        "\n",
        "# # ✅ Example usage:\n",
        "# # captions = batch_caption(frames, HF_API_KEY)\n",
        "\n",
        "# def batch_caption(frames: list[str], hf_token: str) -> dict[int, str]:\n",
        "#     captions = {}\n",
        "#     for idx, frame_path in enumerate(frames):\n",
        "#         blip_caption = caption_image_blip(frame_path, hf_token)\n",
        "#         if blip_caption == \"Caption generation failed\":\n",
        "#             # Try CLIP Interrogator as fallback\n",
        "#             clip_caption = image_to_prompt_clip(frame_path)\n",
        "#             captions[idx] = clip_caption\n",
        "#         else:\n",
        "#             captions[idx] = blip_caption\n",
        "#     return captions\n",
        "\n",
        "def batch_caption(frames: list[str], hf_token: str) -> dict[int, str]:\n",
        "    \"\"\"\n",
        "    Captions images using BLIP-2 (via HF API), with fallback to CLIP Interrogator.\n",
        "    Returns dict mapping frame index to caption text.\n",
        "    \"\"\"\n",
        "    captions = {}\n",
        "    for idx, frame_path in enumerate(frames):\n",
        "        if not os.path.exists(frame_path):\n",
        "            print(f\"[Missing Frame] Skipping: {frame_path}\")\n",
        "            captions[idx] = \"Frame missing\"\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            blip_caption = caption_image_blip(frame_path, hf_token)\n",
        "        except Exception as e:\n",
        "            print(f\"[BLIP Error] {frame_path}: {e}\")\n",
        "            blip_caption = \"Caption generation failed\"\n",
        "\n",
        "        if blip_caption == \"Caption generation failed\":\n",
        "            try:\n",
        "                clip_caption = image_to_prompt_clip(frame_path)\n",
        "            except Exception as e:\n",
        "                print(f\"[CLIP Fallback Error] {frame_path}: {e}\")\n",
        "                clip_caption = \"Caption generation failed\"\n",
        "            captions[idx] = clip_caption\n",
        "        else:\n",
        "            captions[idx] = blip_caption\n",
        "\n",
        "    return captions\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q2_iTbIdHxF"
      },
      "source": [
        "Tip: BLIP-2 outputs a short descriptive caption, which you’ll later refine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78GSpgZJdUmn"
      },
      "source": [
        "B. Image→Prompt via CLIP Interrogator (Local, GPU/CPU)\n",
        "If you want a more precise SD-style prompt rather than just a caption, use a local CLIP Interrogator (requires GPU for speed, but CPU can work slowly). On Colab with GPU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FRmurn8dFiX"
      },
      "outputs": [],
      "source": [
        "# Colab cell 8B: Install CLIP Interrogator (only on GPU-enabled notebook)\n",
        "!pip install git+https://github.com/pharmapsychotic/clip-interrogator.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suOgErrYeYnM"
      },
      "outputs": [],
      "source": [
        "# # Colab cell 8C: Use CLIP Interrogator\n",
        "# from clip_interrogator import Config, Interrogator\n",
        "\n",
        "# # Set up once\n",
        "# ci = Interrogator(Config(clip_model_name=\"ViT-L-14/openai\"))\n",
        "\n",
        "# def image_to_prompt_clip(frame_path: str) -> str:\n",
        "#     \"\"\"\n",
        "#     Reverse-engineer an SD-like prompt from an image frame using CLIP Interrogator.\n",
        "#     \"\"\"\n",
        "#     prompt = ci.interrogate(frame_path)\n",
        "#     return prompt\n",
        "\n",
        "# # Example:\n",
        "# # refined_prompt = image_to_prompt_clip(frames[0])\n",
        "\n",
        "from clip_interrogator import Config, Interrogator\n",
        "\n",
        "# Set up globally once\n",
        "ci = Interrogator(Config(clip_model_name=\"ViT-L-14/openai\"))\n",
        "\n",
        "# def image_to_prompt_clip(frame_path: str) -> str:\n",
        "#     \"\"\"\n",
        "#     Reverse-engineer an SD-like prompt from an image frame using CLIP Interrogator.\n",
        "#     If it fails, logs and returns fallback.\n",
        "#     \"\"\"\n",
        "#     try:\n",
        "#         prompt = ci.interrogate(frame_path)\n",
        "#         return prompt\n",
        "#     except Exception as e:\n",
        "#         print(f\"[CLIP Error] Failed on {frame_path}: {e}\")\n",
        "#         return \"Caption generation failed\"\n",
        "\n",
        "def image_to_prompt_clip(frame_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate a descriptive prompt using CLIP Interrogator.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(frame_path):\n",
        "            print(f\"[CLIP Error] File not found: {frame_path}\")\n",
        "            return \"File missing\"\n",
        "\n",
        "        return ci.interrogate(frame_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[CLIP Error] {frame_path}: {e}\")\n",
        "        return \"Caption generation failed\"\n",
        "\n",
        "\n",
        "\n",
        "# from clip_interrogator import Config, Interrogator\n",
        "\n",
        "# ci = Interrogator(Config(clip_model_name=\"ViT-B-16\"))  # adjust as needed\n",
        "\n",
        "# def image_to_prompt_clip(frame_path: str) -> str:\n",
        "#     try:\n",
        "#         return ci.interrogate(frame_path)\n",
        "#     except Exception as e:\n",
        "#         print(f\"[CLIP Error] on {frame_path}: {e}\")\n",
        "#         return \"Caption generation failed\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY4VwbV5ex7s"
      },
      "source": [
        "When to use?\n",
        "\n",
        "If you need exactly SD-style prompts (e.g., “ultra-detailed, cinematic lighting, 4k…”).\n",
        "\n",
        "If your HP 15s has no GPU, skip CLIP and stay with BLIP-2 captions or HF Image2Prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQQ9osJYe-Wk"
      },
      "source": [
        "7. Scene / Frame-by-Frame LLM Analysis\n",
        "Now that you have either:\n",
        "\n",
        "captions (BLIP-2)\n",
        "\n",
        "refined prompts (CLIP Interrogator)\n",
        "\n",
        "And optionally transcript, you can call an LLM to produce:\n",
        "\n",
        "Final refined prompt\n",
        "\n",
        "Scene narration summary (if audio)\n",
        "\n",
        "Scene meaning/storyline\n",
        "\n",
        "Optional style/tone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNqSXygEfAC9"
      },
      "outputs": [],
      "source": [
        "# # A. Crafting the LLM Prompt\n",
        "# # Colab cell 9A: Create a prompt for each scene/frame\n",
        "# def create_llm_prompt(\n",
        "#     scene_index: int,\n",
        "#     raw_caption: str,\n",
        "#     refined_prompt: str,\n",
        "#     transcript_segment: str,\n",
        "#     user_context: str = \"\"\n",
        "# ) -> str:\n",
        "#     \"\"\"\n",
        "#     Returns a single string prompt to send to the LLM for one scene/frame.\n",
        "#     \"\"\"\n",
        "#     return f\"\"\"\n",
        "# You are a world-class AI content strategist and video analyst.\n",
        "\n",
        "# User Context: {user_context or 'None'}\n",
        "\n",
        "# Scene #{scene_index + 1}:\n",
        "# - Raw Caption: \"{raw_caption}\"\n",
        "# - Refined Prompt (image2prompt style): \"{refined_prompt}\"\n",
        "# - Transcript (audio) snippet: \"{transcript_segment or 'No audio'}\"\n",
        "\n",
        "# Please return (as JSON):\n",
        "# 1. caption: A polished description of the scene.\n",
        "# 2. prompt: The most accurate, detailed Stable-Diffusion style prompt for generating a similar image.\n",
        "# 3. narration: A short voiceover or dialogue based on the transcript/snippet.\n",
        "# 4. meaning: What this scene represents, its significance in the overall story.\n",
        "# 5. scene_type: One of [\"action\",\"static\",\"transition\",\"dialogue\"].\n",
        "\n",
        "# Do NOT return any extra text. Only return valid JSON.\n",
        "# \"\"\"\n",
        "# ## For English video\n",
        "# def create_llm_prompt(\n",
        "#     scene_index: int,\n",
        "#     raw_caption: str,\n",
        "#     refined_prompt: str,\n",
        "#     transcript_segment: str,\n",
        "#     user_context: str = \"\"\n",
        "# ) -> str:\n",
        "#     return f\"\"\"\n",
        "# You are a world-class AI content strategist and video analyst.\n",
        "\n",
        "# User Context: {user_context or 'None'}\n",
        "\n",
        "# Scene #{scene_index + 1}:\n",
        "# - Raw Caption: \"{raw_caption}\"\n",
        "# - Refined Prompt (image2prompt style): \"{refined_prompt}\"\n",
        "# - Transcript (audio) snippet: \"{transcript_segment or 'No audio'}\"\n",
        "\n",
        "# Please return ONLY valid JSON like this:\n",
        "# {{\n",
        "#   \"caption\": \"...\",\n",
        "#   \"prompt\": \"...\",\n",
        "#   \"narration\": \"...\",\n",
        "#   \"meaning\": \"...\",\n",
        "#   \"scene_type\": \"static\"  // one of [\"action\",\"static\",\"transition\",\"dialogue\"]\n",
        "# }}\n",
        "\n",
        "# DO NOT return any explanation or text outside the JSON.\n",
        "# \"\"\"\n",
        "## for hindi video\n",
        "def create_llm_prompt(\n",
        "    scene_index: int,\n",
        "    raw_caption: str,\n",
        "    refined_prompt: str,\n",
        "    transcript_segment: str,\n",
        "    user_context: str = \"\"\n",
        ") -> str:\n",
        "    return f\"\"\"\n",
        "You are a world-class AI content strategist and video analyst.\n",
        "\n",
        "User Context: {user_context or 'None'}\n",
        "\n",
        "Scene #{scene_index + 1}:\n",
        "- Raw Caption: \"{raw_caption}\"\n",
        "- Refined Prompt (image2prompt style): \"{refined_prompt}\"\n",
        "- Transcript (audio) snippet (may be in Hindi): \"{transcript_segment or 'No audio'}\"\n",
        "\n",
        "Your task is to analyze the scene. If the transcript is in Hindi, translate it to English.\n",
        "Return all fields below ONLY in English.\n",
        "\n",
        "Please return ONLY valid JSON like this:\n",
        "{{\n",
        "  \"caption\": \"...\",\n",
        "  \"prompt\": \"...\",\n",
        "  \"narration\": \"...\",\n",
        "  \"meaning\": \"...\",\n",
        "  \"scene_type\": \"static\"  // one of [\"action\",\"static\",\"transition\",\"dialogue\"]\n",
        "}}\n",
        "\n",
        "DO NOT return any explanation or text outside the JSON.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tu_LVGSIfdBL"
      },
      "outputs": [],
      "source": [
        "# # B. Call OpenRouter (Free GPT-3.5)\n",
        "# # Colab cell 9B: Send prompt to OpenRouter\n",
        "# def call_openrouter_llm(system_prompt: str, user_prompt: str) -> dict:\n",
        "#     \"\"\"\n",
        "#     Calls OpenRouter’s GPT-3.5-turbo (free tier) to process the combined prompt.\n",
        "#     Returns the parsed JSON result.\n",
        "#     \"\"\"\n",
        "#     url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "#     headers = {\n",
        "#         \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "#         \"Content-Type\": \"application/json\"\n",
        "#     }\n",
        "#     payload = {\n",
        "#         \"model\": \"openai/gpt-3.5-turbo\",\n",
        "#         \"messages\": [\n",
        "#             {\"role\": \"system\", \"content\": system_prompt},\n",
        "#             {\"role\": \"user\", \"content\": user_prompt}\n",
        "#         ]\n",
        "#     }\n",
        "#     resp = requests.post(url, headers=headers, json=payload)\n",
        "#     if resp.status_code != 200:\n",
        "#         raise Exception(f\"OpenRouter error {resp.status_code}: {resp.text}\")\n",
        "#     data = resp.json()\n",
        "#     # data[\"choices\"][0][\"message\"][\"content\"] is the JSON string\n",
        "#     content = data[\"choices\"][0][\"message\"][\"content\"]\n",
        "#     return json.loads(content)\n",
        "\n",
        "# # Example usage for one scene:\n",
        "# # sys_msg = \"You are a world-class AI content strategist…\"\n",
        "# # user_msg = create_llm_prompt(0, captions[0], refined_prompts[0], transcript_snippets[0], \"My context\")\n",
        "# # analysis = call_openrouter_llm(sys_msg, user_msg)\n",
        "# # → analysis is a dict with keys: caption, prompt, narration, meaning, scene_type\n",
        "\n",
        "# Colab cell 9B: Local LLM using Mistral in transformers (no API or credit needed)\n",
        "# !pip install -q transformers accelerate bitsandbytes\n",
        "\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "# import torch\n",
        "\n",
        "# # Load Mistral-7B-Instruct model (quantized for Colab GPU)\n",
        "# model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_id,\n",
        "#     device_map=\"auto\",\n",
        "#     torch_dtype=torch.float16,\n",
        "#     load_in_4bit=True\n",
        "# )\n",
        "# generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# # Updated local LLM call function (replaces call_openrouter_llm)\n",
        "# def call_local_llm(system_prompt: str, user_prompt: str) -> dict:\n",
        "#     \"\"\"\n",
        "#     Calls a local open-source LLM (e.g., Mistral) for processing.\n",
        "#     Returns a parsed JSON dict with the structured scene analysis.\n",
        "#     \"\"\"\n",
        "#     prompt = f\"{system_prompt.strip()}\\n\\n{user_prompt.strip()}\"\n",
        "#     output = generator(prompt, max_new_tokens=512, do_sample=True, temperature=0.7)[0][\"generated_text\"]\n",
        "\n",
        "#     # Try to extract JSON substring\n",
        "#     try:\n",
        "#         json_str = output[output.index(\"{\"):output.rindex(\"}\") + 1]\n",
        "#         return json.loads(json_str)\n",
        "#     except Exception:\n",
        "#         print(\"⚠️ Could not parse JSON. Returning raw output.\")\n",
        "#         return {\"response\": output}\n",
        "\n",
        "\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "# import torch\n",
        "# from huggingface_hub import login\n",
        "\n",
        "# # Paste your Hugging Face token here\n",
        "# login(\"#add your api\")\n",
        "# # ✅ Use an open-access variant\n",
        "# model_id = \"mistralai/Mistral-7B-v0.1\"  # No access gate\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_id,\n",
        "#     device_map=\"auto\",\n",
        "#     torch_dtype=torch.float16,\n",
        "#     load_in_4bit=True\n",
        "# )\n",
        "# generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# def call_local_llm(system_prompt: str, user_prompt: str) -> dict:\n",
        "#     prompt = f\"{system_prompt.strip()}\\n\\n{user_prompt.strip()}\"\n",
        "#     output = generator(prompt, max_new_tokens=512, do_sample=True, temperature=0.7)[0][\"generated_text\"]\n",
        "#     try:\n",
        "#         json_str = output[output.index(\"{\"):output.rindex(\"}\") + 1]\n",
        "#         return json.loads(json_str)\n",
        "#     except Exception:\n",
        "#         print(\"⚠️ Could not parse JSON. Returning raw output.\")\n",
        "#         return {\"response\": output}\n",
        "\n",
        "# !pip install -U bitsandbytes transformers accelerate\n",
        "\n",
        "# !pip uninstall -y bitsandbytes\n",
        "# !pip install --no-cache-dir --force-reinstall bitsandbytes==0.41.1\n",
        "# !pip install --upgrade transformers accelerate\n",
        "\n",
        "# Install required packages\n",
        "# Install required packages\n",
        "\n",
        "#latest\n",
        "\n",
        "# !pip install -U transformers accelerate\n",
        "\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "# from huggingface_hub import login\n",
        "# import torch\n",
        "# import json\n",
        "\n",
        "# # Hugging Face login (replace with your token)\n",
        "# login(\"#add your api\")\n",
        "\n",
        "# # Load model\n",
        "# model_id = \"microsoft/phi-2\"\n",
        "\n",
        "# # Load tokenizer and model in float16 for GPU\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     model_id,\n",
        "#     device_map=\"auto\",\n",
        "#     torch_dtype=torch.float16\n",
        "# )\n",
        "\n",
        "# # Create text generation pipeline\n",
        "# generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# # Define generation function\n",
        "# def call_local_llm(system_prompt: str, user_prompt: str) -> dict:\n",
        "#     prompt = f\"{system_prompt.strip()}\\n\\n{user_prompt.strip()}\"\n",
        "#     output = generator(prompt, max_new_tokens=512, do_sample=True, temperature=0.7)[0][\"generated_text\"]\n",
        "#     try:\n",
        "#         json_str = output[output.index(\"{\"):output.rindex(\"}\") + 1]\n",
        "#         return json.loads(json_str)\n",
        "#     except Exception:\n",
        "#         print(\"⚠️ Could not parse JSON. Returning raw output.\")\n",
        "#         return {\"response\": output}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install and Login\n",
        "!pip install -U transformers accelerate bitsandbytes\n",
        "from huggingface_hub import login\n",
        "login(\"\")  # Use your actual HF token\n",
        "\n",
        "# 2. Import and load Mistral-7B-Instruct or Zephyr\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "import json\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"  # or \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_8bit=True  # Optional: saves VRAM\n",
        ")\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# 3. JSON-capable generation wrapper\n",
        "def call_local_llm(system_prompt: str, user_prompt: str) -> dict:\n",
        "    full_prompt = f\"<s>[INST] <<SYS>>\\n{system_prompt.strip()}\\n<</SYS>>\\n\\n{user_prompt.strip()} [/INST]\"\n",
        "    result = generator(full_prompt, max_new_tokens=512, temperature=0.7)[0][\"generated_text\"]\n",
        "\n",
        "    try:\n",
        "        json_str = result[result.index(\"{\"):result.rindex(\"}\") + 1]\n",
        "        return json.loads(json_str)\n",
        "    except Exception:\n",
        "        print(\"⚠️ Could not parse JSON. Returning raw output.\")\n",
        "        return {\"response\": result}\n"
      ],
      "metadata": {
        "id": "USarRxBJFT_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSetUp6Pcz3d"
      },
      "source": [
        "Why OpenRouter?\n",
        "\n",
        "They offer a free GPT-3.5 endpoint without cost.\n",
        "\n",
        "Enough for a few dozen scenes per day."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "E3cg6hoAgHXz"
      },
      "outputs": [],
      "source": [
        "# Colab cell 10: Main pipeline function\n",
        "def process_video_pipeline(\n",
        "    source: str,          # either a YouTube URL or local file path\n",
        "    use_audio: bool = True,\n",
        "    frame_interval: int = 2,\n",
        "    scene_mode: str = \"interval\",  # \"interval\" or \"scene_detect\"\n",
        "    user_context: str = \"\"\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    End-to-end pipeline. Returns a structured dict with all scene data.\n",
        "    \"\"\"\n",
        "    # 1. Ingest video\n",
        "    if source.startswith(\"http\"):\n",
        "        video_path = download_video(source)\n",
        "    else:\n",
        "        video_path = copy_local_video(source)\n",
        "\n",
        "    # 2. Extract audio if requested\n",
        "    transcript = \"\"\n",
        "    if use_audio:\n",
        "        audio_path = extract_audio(video_path)\n",
        "        # You can choose local or HF transcription:\n",
        "        transcript = transcribe_audio_local(audio_path)  # or transcribe_audio_hf(audio_path)\n",
        "\n",
        "    # 3. Extract frames\n",
        "    if scene_mode == \"scene_detect\":\n",
        "        frames, timestamps = extract_frames_scenes(video_path)\n",
        "    else:\n",
        "        frames = extract_frames_interval(video_path, interval=frame_interval)\n",
        "        timestamps = [idx * frame_interval for idx in range(len(frames))]\n",
        "    # 4. Caption frames\n",
        "    captions = batch_caption(frames, HF_API_KEY)  # uses BLIP-2 via HF Inference API\n",
        "\n",
        "    # 5. (Optional) Refine prompts via CLIP Interrogator if GPU available:\n",
        "    # try:\n",
        "    #     refined_prompts = {\n",
        "    #         idx: image_to_prompt_clip(fp) for idx, fp in enumerate(frames)\n",
        "    #     }\n",
        "    # except Exception:\n",
        "    #     refined_prompts = captions  # fallback: use raw captions as prompts\n",
        "    refined_prompts = {}\n",
        "    for idx, fp in enumerate(frames):\n",
        "        refined = image_to_prompt_clip(fp)\n",
        "        if refined == \"Caption generation failed\":\n",
        "            refined = captions.get(idx, \"No caption available\")\n",
        "        refined_prompts[idx] = refined\n",
        "\n",
        "\n",
        "    # 6. Split transcript into segments aligned to frames (approximate)\n",
        "    transcript_snippets = []\n",
        "    if use_audio:\n",
        "        words = transcript.split()\n",
        "        words_per_frame = max(1, len(words) // len(frames))\n",
        "        for i in range(len(frames)):\n",
        "            start = i * words_per_frame\n",
        "            end = (i + 1) * words_per_frame\n",
        "            snippet = \" \".join(words[start:end])\n",
        "            transcript_snippets.append(snippet)\n",
        "    else:\n",
        "        transcript_snippets = [\"\"] * len(frames)\n",
        "\n",
        "    # 7. LLM analysis using local model\n",
        "    # scenes = []\n",
        "    # for idx, fp in enumerate(frames):\n",
        "    #     raw_cap = captions[idx]\n",
        "    #     ref_prompt = refined_prompts[idx]\n",
        "    #     snippet = transcript_snippets[idx]\n",
        "    #     user_msg = create_llm_prompt(idx, raw_cap, ref_prompt, snippet, user_context)\n",
        "    #     system_msg = \"You are a top‐tier AI content strategist...\"\n",
        "    #     result = call_local_llm(system_msg, user_msg)\n",
        "    #     scenes.append({\n",
        "    #         \"scene_id\": idx + 1,\n",
        "    #         \"timestamp\": idx * frame_interval,\n",
        "    #         \"frame_path\": fp,\n",
        "    #         \"raw_caption\": raw_cap,\n",
        "    #         \"refined_prompt\": result.get(\"prompt\", ref_prompt),\n",
        "    #         \"narration\": result.get(\"narration\", snippet),\n",
        "    #         \"meaning\": result.get(\"meaning\", \"\"),\n",
        "    #         \"scene_type\": result.get(\"scene_type\", \"static\")\n",
        "    #     })\n",
        "    scenes = []\n",
        "    for idx, fp in enumerate(frames):\n",
        "        try:\n",
        "            raw_cap = captions[idx]\n",
        "            ref_prompt = refined_prompts[idx]\n",
        "            snippet = transcript_snippets[idx]\n",
        "            user_msg = create_llm_prompt(idx, raw_cap, ref_prompt, snippet, user_context)\n",
        "            system_msg = \"You are a top‐tier AI content strategist and video analyst.\"\n",
        "\n",
        "            result = call_local_llm(system_msg, user_msg)\n",
        "\n",
        "            scenes.append({\n",
        "                \"scene_id\": idx + 1,\n",
        "                \"timestamp\": idx * frame_interval,\n",
        "                \"frame_path\": fp,\n",
        "                \"raw_caption\": raw_cap,\n",
        "                \"refined_prompt\": result.get(\"prompt\", ref_prompt),\n",
        "                \"narration\": result.get(\"narration\", snippet),\n",
        "                \"meaning\": result.get(\"meaning\", \"\"),\n",
        "                \"scene_type\": result.get(\"scene_type\", \"static\")\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"[LLM Error] Scene {idx}: {e}\")\n",
        "\n",
        "\n",
        "    # 8. Save structured JSON & return\n",
        "    out = {\n",
        "        \"video_path\": video_path,\n",
        "        \"transcript\": transcript,\n",
        "        \"scenes\": scenes\n",
        "    }\n",
        "    with open(f\"{OUTPUT_DIR}/analysis_result.json\", \"w\") as f:\n",
        "        json.dump(out, f, indent=2)\n",
        "    return out\n",
        "\n",
        "# Example: run pipeline on a YouTube short\n",
        "result = process_video_pipeline(\n",
        "    \"https://youtube.com/shorts/8-jFqdBBV0M?si=SNSHVFvv2zPJOwMz\",\n",
        "    use_audio=False,\n",
        "    frame_interval=2.0,\n",
        "    scene_mode=\"interval\",  # \"interval\" or \"scene_detect : should update in next implementaions\"\n",
        "    user_context=\"I want a cinematic and animatic tone\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfnMRKqQgoVr"
      },
      "source": [
        "What happens here:\n",
        "\n",
        "Downloads or copies video.\n",
        "\n",
        "Extracts audio + transcript.\n",
        "\n",
        "Splits into frames (every 2 s).\n",
        "\n",
        "Captions via BLIP-2.\n",
        "\n",
        "(Optionally) Runs CLIP Interrogator—if GPU is available in Colab.\n",
        "\n",
        "Breaks transcript into equal pieces per frame.\n",
        "\n",
        "Sends each scene’s data to GPT-3.5 via OpenRouter.\n",
        "\n",
        "Saves everything in /output/analysis_result.json (plus all frames under /frames/…)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2thIrRWRg11V"
      },
      "outputs": [],
      "source": [
        "# 9. Download or Clean Up\n",
        "# Colab cell 11: Functions to download or clean output\n",
        "\n",
        "# Download the entire OUTPUT_DIR as .zip for local storage\n",
        "# !zip -r /content/analysis_output.zip {OUTPUT_DIR}\n",
        "\n",
        "# To clean (delete) all files in /videos, /frames, /output:\n",
        "def clean_all():\n",
        "    for d in [VIDEOS_DIR, FRAMES_DIR, OUTPUT_DIR]:\n",
        "        if os.path.exists(d):\n",
        "            shutil.rmtree(d)\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "    print(\"Cleaned all directories.\")\n",
        "\n",
        "# Example:\n",
        "# clean_all()\n",
        "# On Colab, after zip, you can click the file-\n",
        "# -icon on the left panel and download analysis_output.zip."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4ZULwSJhiGa"
      },
      "source": [
        "💡 Additional Enhancements & “Impactful” Add‐Ons\n",
        "Adaptive Frame Interval\n",
        "\n",
        "Instead of a fixed 2 s, adjust interval based on scene complexity:\n",
        "\n",
        "Use scene detection for abrupt changes (PySceneDetect).\n",
        "\n",
        "Fallback to fixed if scene detection fails.\n",
        "\n",
        "Prompt Paraphrasing\n",
        "\n",
        "After obtaining a raw prompt, run a “paraphrase” step (via a second LLM call) to get 3–5 variant prompts (e.g. “fantasy style,” “cinematic,” “noir,” etc.).\n",
        "\n",
        "Emotion/Tone Tagging\n",
        "\n",
        "In your LLM prompt, ask for a tone tag (“Scene is comedic/tense/dark”) and store it.\n",
        "\n",
        "Metadata Export in Multiple Formats\n",
        "\n",
        "Save not only JSON but also:\n",
        "\n",
        "A Markdown storyboard (analysis_result.md)\n",
        "\n",
        "A CSV for easy spreadsheet import\n",
        "\n",
        "Interactive Visualization (in Colab)\n",
        "\n",
        "Display the first few frames inline with their captions and refined prompts:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "from IPython.display import display, Image\n",
        "for i in range(min(5, len(result[\"scenes\"]))):\n",
        "    img_path = result[\"scenes\"][i][\"frame_path\"]\n",
        "    display(Image(filename=img_path, width=320))\n",
        "    print(\"Prompt:\", result[\"scenes\"][i][\"refined_prompt\"])\n",
        "    print(\"---\")\n",
        "Local HTML Export\n",
        "\n",
        "Generate a tiny index.html that loads from /frames/… and displays scene cards (thumbnail + captions), so you can open it offline in any browser.\n",
        "\n",
        "Audio Clip Extraction\n",
        "\n",
        "If audio has multiple speakers or segments, split audio by silence to get per-scene audio clips.\n",
        "\n",
        "Then transcribe individually for better alignment.\n",
        "\n",
        "Model Checkpoint Caching\n",
        "\n",
        "If you run multiple videos in one Colab session, BLIP-2 and Whisper caches will speed up subsequent calls.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}